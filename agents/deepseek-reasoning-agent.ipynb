{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek Reasoning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs locally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install [Ollama](https://ollama.com/) and run the following command to install the DeepSeek R1 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6e9f90f02bb3... 100% ▕████████████████▏ 9.0 GB                         \n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \n",
      "pulling 3c24b0c80794... 100% ▕████████████████▏  488 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ollama pull deepseek-r1:14b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"deepseek-r1:14b\", temperature=0)\n",
    "llm_json_mode = ChatOllama(model=\"deepseek-r1:14b\", temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nThe capital of France is Paris.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = llm.invoke(\"What is the capital of France?\")\n",
    "msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `<think>` tokens are part of the training process, it's hard to get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aspect': 'general information',\n",
      " 'query': 'cats',\n",
      " 'rationale': \"The term 'cats' is broad and can encompass various aspects such \"\n",
      "              'as their biology, behavior, breeds, health, and more. Using the '\n",
      "              'general search query will gather comprehensive information '\n",
      "              'covering all these areas.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "p = \"\"\"Your goal is to generate targeted web search query.\n",
    "\n",
    "The query will gather information related to a specific topic.\n",
    "\n",
    "Topic: cats\n",
    "\n",
    "Return your query as a JSON object:\n",
    "{{\n",
    "    \"query\": \"string\",\n",
    "    \"aspect\": \"string\",\n",
    "    \"rationale\": \"string\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "result = llm_json_mode.invoke(p)\n",
    "query = json.loads(result.content)\n",
    "pprint(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so I'm trying to understand scaling laws for reinforcement learning (RL) models. From what I remember, scaling laws in machine learning generally refer to how performance changes as we adjust certain parameters like model size or the amount of training data. But since I'm new to this, I need to break it down step by step.\\n\\nFirst, RL involves agents interacting with environments to learn optimal behaviors through trial and error. The agent gets rewards or penalties based on its actions, aiming to maximize cumulative reward. So scaling laws here would probably look at how different factors affect the agent's learning efficiency and performance.\\n\\nI think model size is a big factor. Bigger models might have more parameters, allowing them to capture more complex patterns. But does that always translate to better performance? Maybe up to a point, but perhaps after a certain size, adding more parameters doesn't help as much or even makes things worse due to overfitting.\\n\\nThen there's the environment complexity. If the environment is simple, maybe smaller models can handle it well. But for complex environments with many states and actions, larger models might be necessary to capture all possible nuances. So scaling laws here would probably show that model size needs to match environmental complexity.\\n\\nThe number of interactions or training steps must matter too. More data from interacting with the environment should help the agent learn better policies. But there's a balance; too much data without proper structure might not be useful, and it could take longer to train. So scaling laws would likely indicate that up to a certain point, more interactions improve performance, but after that, returns diminish.\\n\\nI also recall something about sample efficiency in RL. Some algorithms are better at learning with fewer samples. Scaling laws here might compare how different algorithms scale with the number of interactions, showing which ones are more efficient as data increases.\\n\\nThe reward magnitude and delay probably play a role too. If rewards are sparse or delayed, it's harder for agents to learn because they don't get immediate feedback. So scaling laws could show that under these conditions, performance doesn't scale as well with model size or training time compared to cases with dense rewards.\\n\\nCurriculum learning is another concept where the environment starts simple and becomes more complex over time. Scaling laws here might suggest that following a curriculum helps agents learn better, especially for larger models, by gradually introducing complexity.\\n\\nMulti-agent systems add another layer. In these setups, scaling laws would consider how interactions between agents affect overall performance. More agents could lead to more complex dynamics, so the scaling behavior might change depending on whether agents are cooperative or competitive.\\n\\nI should also think about the relationship between model capacity and sample size. If you have a very large model but not enough data, it might overfit. So scaling laws would probably show that for optimal performance, model size needs to be balanced with the amount of training data available.\\n\\nLastly, generalization is key. RL models need to perform well not just on seen scenarios but also novel ones. Scaling laws here might indicate how different scaling approaches affect the model's ability to generalize, showing whether larger models or more data leads to better out-of-distribution performance.\\n\\nPutting it all together, scaling laws in RL likely involve understanding how model size, training interactions, environment complexity, reward characteristics, and multi-agent dynamics interact to influence learning outcomes. Each factor has its own scaling behavior, and optimizing these can lead to better-performing agents.\\n</think>\\n\\nScaling laws in reinforcement learning (RL) describe how various factors influence the performance of RL models as they are scaled up or adjusted. Here's a structured summary based on the thought process:\\n\\n1. **Model Size**: Larger models with more parameters can capture complex patterns, but there is a point where additional parameters do not improve performance and may lead to overfitting.\\n\\n2. **Environment Complexity**: The complexity of the environment dictates the required model size. Simpler environments may be managed by smaller models, while complex ones need larger models to handle nuances.\\n\\n3. **Training Interactions**: More interactions with the environment generally enhance learning, but there is a diminishing return after a certain threshold. This balance affects training efficiency and duration.\\n\\n4. **Sample Efficiency**: Different RL algorithms vary in how effectively they use data. Scaling laws highlight which algorithms scale better with more interactions, emphasizing efficiency.\\n\\n5. **Reward Characteristics**: Sparse or delayed rewards hinder learning. Scaling laws show that performance may not scale as effectively under these conditions compared to dense rewards.\\n\\n6. **Curriculum Learning**: Gradually increasing environmental complexity can improve learning outcomes, particularly for larger models, by easing into complex tasks.\\n\\n7. **Multi-Agent Systems**: The dynamics between agents (cooperative or competitive) affect scaling behavior. More agents introduce complexity that needs careful management.\\n\\n8. **Model Capacity and Data**: Balancing model size with training data is crucial to prevent overfitting. Optimal performance requires a harmonious relationship between the two.\\n\\n9. **Generalization**: Scaling laws consider how models generalize to novel scenarios. Larger models or more data can enhance out-of-distribution performance, but this depends on the scaling approach.\\n\\nIn conclusion, understanding these factors and their interactions is key to optimizing RL model performance through effective scaling strategies.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = llm.invoke(\"Give me a summary of scaling laws for RL models.\")\n",
    "msg.content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
